This study explores the parallelization of Convolutional Neural Networks (CNNs)
using three distinct approaches: Message Passing Interface (MPI), Open MultiProcessing (OpenMP), and CUDA C. We analyze the performance, efficiency, and
scalability of each method across various CNN layers, providing insights into their
strengths and limitations. Our findings reveal significant speedups, particularly with
GPU-based parallelization, while also highlighting challenges in managing communication overhead and optimizing kernel executions.
